---
title: "Spring 2023 STAT 707 Chapter 7 Homework"
author: "Reina Li"
output: pdf_document
---

# Chapter 7: 7.5, 7.6, 7.9, 7.18, 7.30, 7.34

## 7.5. Refer to Patient satisfaction Problem 6.15.

```{r problem_setup1, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Import data set
df <- read.table("CH06PR15.txt")
# Set column names
colnames(df) <- c("Y", "X1", "X2", "X3")
```

### a. Obtain the analysis of variance table that decomposes the regression sum of squares into extra sum of squares associated with $X_2$; with $X_1$, given $X_2$; and with $X_3$, given $X_2$ and $X_1$.

```{r 7_5a, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Fit regression model
model <- lm(Y ~ X2 + X1 + X3, data = df)
# ANOVA table
anova(model)
# SSR(X1,X2,X3)
SSR <- sum(anova(model)[1:3,2]); SSR
# MSR(X1,X2,X3)
MSR <- SSR / 3; MSR
# SSE(X1,X2,X3)
SSE <- anova(model)[4,2]; SSE
# MSE(X1,X2,X3)
MSE <- anova(model)[4,3]; MSE
```

### b. Test whether $X_3$ can be dropped from the regression model given that $X_1$ and $X_2$ are retained. Use the $F$* test statistic and level of significance .025. State the alternatives, decision rule, and conclusion. What is the P-value of the test?

```{r 7_5b, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Full model : lm(Y ~ X2 + X1 + X3, data = df)
# Fit reduced model without X3: Y_i = B0 + B1*X_i1 + B2*X_i2 + e_i
reduced_model <- lm(Y ~ X2 + X1, data = df)
anova(reduced_model, model)
# Critical value F
alpha <- 0.025
qf(1-alpha, 1, 42)
```

Alternatives:    
$H_0$ : $\beta_3=0$  
$H_a$ : $\beta_3\neq0$

Test statistic:    
$F$* = 3.5997    
$F$(0.975,1,42) = 5.403859

Decision rule:     
If $F$* $\le$ $F$(0.975,1,42), conclude $H_0$  
If $F$* $>$ $F$(0.975,1,42), conclude $H_a$

Conclusion:    
$F$* $\le$ $F$(0.975,1,42)     
Conclude $H_0$. Fail to reject the null hypothesis $H_0$. $X_3$ can be dropped from the regression model that already contains $X_1$ and $X_2$.

P-value: 0.06468

## 7.6. Refer to Patient satisfaction Problem 6.15. Test whether both $X_2$ and $X_3$ can be dropped from the regression model given that $X_1$ is retained. Use $\alpha=.025$. State the alternatives, decision rule, and conclusion. What is the P-value of the test?

```{r 7_6, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Full model : lm(Y ~ X2 + X1 + X3, data = df)
# Fit reduced model without X2 and X3: Y_i = B0 + B1*X_i1 + e_i
reduced_model <- lm(Y ~ X1, data = df)
anova(reduced_model, model)
# Critical value F
alpha <- 0.025
qf(1-alpha, 2, 42)
```

Alternatives:    
$H_0$ : $\beta_2=\beta_3=0$  
$H_a$ : not both $\beta_2$ and $\beta_3$ equal zero

Test statistic:    
$F$* = 4.1768    
$F$(0.975,2,42) = 4.03271

Decision rule:     
If $F$* $\le$ $F$(0.975,2,42), conclude $H_0$  
If $F$* $>$ $F$(0.975,2,42), conclude $H_a$

Conclusion:    
$F$* $>$ $F$(0.975,2,42)     
Conclude $H_a$. Reject the null hypothesis $H_0$. Don't drop $X_2$ and $X_3$ from the regression model that already contains $X_1$.

P-value: 0.02216

## 7.9. Refer to Patient satisfaction Problem 6.15. Test whether $\beta_1=-1.0$ and $\beta_2=0$; use $\alpha=.025$. State the alternatives, full and reduced models, decision rule, and conclusion.

```{r 7_9, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Full model : lm(Y ~ X2 + X1 + X3, data = df)
# Fit reduced model: Y_i = B0 - X_i1 + B3*X_i3 + e_i
reduced_model <- lm(Y + X1 ~ X3, data = df)
anova(reduced_model, model)
# SSE(R)
SSE_R <- 4427.7; SSE_R
# df(R)
df_R <- 44; df_R
# SSE(F)
SSE_F <- SSE; SSE_F
# df(F)
df_F <- 42; df_F
# Test statistic F* = (SSR(R)-SSE(F) / df(R)-df(F)) / SSE(F) / df(F)
test_stat <- ((SSE_R - SSE_F) / (df_R - df_F)) / (SSE_F / df_F); test_stat
# Critical value F
alpha <- 0.025
qf(1-alpha, 2, 42)
```

Alternatives:    
$H_0$ : $\beta_1=-1.0$, $\beta_2=0$  
$H_a$ : not both equalities in $H_0$ hold

Test statistic:    
$F$* = 0.8840166     
$F$(0.975,2,42) = 4.03271

Decision rule:     
If $F$* $\le$ $F$(0.975,2,42), conclude $H_0$  
If $F$* $>$ $F$(0.975,2,42), conclude $H_a$

Conclusion:    
$F$* $\le$ $F$(0.975,2,42)     
Conclude $H_0$. Fail to reject the null hypothesis $H_0$.

## 7.18. Refer to Patient satisfaction Problem 6.15.

### a. Transform the variables by means of the correlation transformation (7.44) and fit the standardized regression model (7.45).

```{r 7_18a, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Create data frame for transformed data
transformed_df <- data.frame(matrix(ncol = 4, nrow = 46))
colnames(transformed_df) <- c("Y_star", "X1_star", "X2_star", "X3_star")
# Standardize variables
y_bar <- mean(df$Y)
s_y <- sd(df$Y)
x_1bar <- mean(df$X1)
s_x1 <- sd(df$X1)
x_2bar <- mean(df$X2)
s_x2 <- sd(df$X2)
x_3bar <- mean(df$X3)
s_x3 <- sd(df$X3)
for (i in 1:46) {
  transformed_df[i,1] <- (1/sqrt(46-1)) * ((df$Y[i]-y_bar)/s_y)
  transformed_df[i,2] <- (1/sqrt(46-1)) * ((df$X1[i]-x_1bar)/s_x1)
  transformed_df[i,3] <- (1/sqrt(46-1)) * ((df$X2[i]-x_2bar)/s_x2)
  transformed_df[i,4] <- (1/sqrt(46-1)) * ((df$X3[i]-x_3bar)/s_x3)
}
# Fit standardized regression model
sd_model <- lm(Y_star ~ X1_star + X2_star + X3_star, data = transformed_df)
summary(sd_model)
```

The estimated standardized regression function is: $\hat{Y}^*=$ `r round(sd_model$coefficients[2][[1]],4)`$X_1^*$ + `r round(sd_model$coefficients[3][[1]],4)`$X_2^*$ + `r round(sd_model$coefficients[4][[1]],4)`$X_3^*$

### b. Calculate the coefficients of determination between all pairs of predictor variables. Do these indicate that it is meaningful here to consider the standardized regression coefficients as indicating the effect of one predictor variable when the others are held constant?

```{r 7_18b, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Square the correlation matrix
rsq_matrix <- (cor(transformed_df))^2; rsq_matrix
# Coefficient of determination between X1* and X2*
rsq_matrix[2,3]
# Coefficient of determination between X1* and X3*
rsq_matrix[2,4]
# Coefficient of determination between X2* and X3*
rsq_matrix[3,4]
```

Yes, these do indicate that it is meaningful here to consider the standardized regression coefficients as indicating the effect of one predictor variable when the others are held constant because the predictor variables are not highly correlated.

### c. Transform the estimated standardized regression coefficients by means of (7.53) back to the ones for the fitted regression model in the original variables. Verify that they are the same as the ones obtained in Problem 6.15c.

```{r 7_18c, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
cor1 <- cor(transformed_df)[2:4,2:4];
cor2 <- cor(transformed_df)[2:4,1];
b_star <- solve(cor1) %*% cor2; b_star
# b1*
b1_star <- b_star[1]; b1_star
# b2*
b2_star <- b_star[2]; b2_star
# b3*
b3_star <- b_star[3]; b3_star
# b1 = (sY/s1)b1*
b1 <- (s_y/s_x1)*b1_star; b1
# b2 = (sY/s2)b2*
b2 <- (s_y/s_x2)*b2_star; b2
# b3 = (sY/s3)b3*
b3 <- (s_y/s_x3)*b3_star; b3
# b0 = ybar - b1*x1bar - b2*x2bar - b3*x3bar
b0 <- y_bar - b1*x_1bar - b2*x_2bar - b3*x_3bar; b0
```
Standardized regression model: $\hat{Y}^*=$ `r round(sd_model$coefficients[2][[1]],4)`$X_1^*$ + `r round(sd_model$coefficients[3][[1]],4)`$X_2^*$ + `r round(sd_model$coefficients[4][[1]],4)`$X_3^*$

Fitted regression model in the original variables: $\hat{Y}=$ `r round(b0,4)` + `r round(b1,4)`$X_1$ + `r round(b2,4)`$X_2$ + `r round(b3,4)`$X_3$

## 7.30. Refer to Brand preference Problem 6.5.

```{r problem_setup2, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Import data set
df <- read.table("CH06PR05.txt")
# Set column names
colnames(df) <- c("Y", "X1", "X2")
```

### a. Regress $Y$ on $X_{2}$ using simple linear regression model (2.1) and obtain the residuals.

```{r 7_30a, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Fit model
model_1 <- lm(Y ~ X2, data = df)
summary(model_1)
# Residuals
resid_1 <- summary(model_1)$residuals; resid_1
```

Model: $\hat{Y}=$ `r round(model_1$coefficients[1][[1]],4)` + `r round(model_1$coefficients[2][[1]],4)`$X_2$

### b. Regress $X_{1}$ on $X_{2}$ using simple linear regression model (2.1) and obtain the residuals.

```{r 7_30b, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Fit model
model_2 <- lm(X1 ~ X2, data = df)
summary(model_2)
# Residuals
resid_2 <- summary(model_2)$residuals; resid_2
```

Model: $\hat{X_1}=$ `r round(model_2$coefficients[1][[1]],4)`

### c. Calculate the coefficient of simple correlation between the two sets of residuals and show that it equals $r_{Y1|2}$.

```{r 7_30c, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Coefficient of simple correlation
simple_cor <- cor(resid_1, resid_2); simple_cor
# Fit model
model <- lm(Y ~ X2 + X1, data = df)
# ANOVA table
anova(model)
# SSR(X1|X2)
SSR_x1x2 <- anova(model)[2,2]; SSR_x1x2
SSE_x2 <- anova(model)[3,2]; SSE_x2
# SSR(X1|X2) = SSE(X2) - SSE(X1,X2)
# SSE(X2) = SSR(X1|X2) + SSE(X1,X2)
# R^2_{Y1|2} = SSR(X1|X2) / SSE(X2)
coeff_det <- SSR_x1x2 / (SSR_x1x2 + SSE_x2); coeff_det
```

## 7.34. Refer to the work crew productivity example in Table 7.6.

```{r problem_setup3, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# Create data frame of data
df <- data.frame(X1 = c(4,4,4,4,6,6,6,6),
                 X2 = c(2,2,3,3,2,2,3,3),
                 Y = c(42,39,48,51,49,53,61,60))
# Create data frame for transformed data
transformed_df <- data.frame(matrix(ncol = 3, nrow = 8))
colnames(transformed_df) <- c("Y_star", "X1_star", "X2_star")
# Standardize variables
y_bar <- mean(df$Y)
s_y <- sd(df$Y)
x_1bar <- mean(df$X1)
s_x1 <- sd(df$X1)
x_2bar <- mean(df$X2)
s_x2 <- sd(df$X2)
for (i in 1:8) {
  transformed_df[i,1] <- (1/sqrt(8-1)) * ((df$Y[i]-y_bar)/s_y)
  transformed_df[i,2] <- (1/sqrt(8-1)) * ((df$X1[i]-x_1bar)/s_x1)
  transformed_df[i,3] <- (1/sqrt(8-1)) * ((df$X2[i]-x_2bar)/s_x2)
}
# Fit model
model <- lm(Y_star ~ X1_star + X2_star, data = transformed_df)
anova(model)
```

### a. For the variables transformed according to (7.44), obtain:

#### (1) X'X

```{r 7_34a1, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# X'X = r_{XX}
cor1 <- cor(transformed_df)[2:3,2:3]; cor1
```

#### (2) X'Y

```{r 7_34a2, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# X'Y = r_{XY}
cor2 <- cor(transformed_df)[2:3,1]; cor2
```

#### (3) b

```{r 7_34a3, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# b = (X'X)^-1 * X'Y
b <- solve(cor1) %*% cor2; b
```

#### (4) s$^2${b}

```{r 7_34a4, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# s^2{b} = (s*)^2 * r_{xx}^-1
anova(model)[3,3] * solve(cor1)
```

### b. Show that the standardized regression coefficients obtained in part (a3) are related to the regression coefficients for the regression model in the original variables according to (7.53).

```{r 7_34b, echo = TRUE, include = TRUE, results = TRUE, warning = FALSE, message = FALSE}
# b1 = (sY/s1)b1*
# b1* = (s1/sY)b1
b1_star <- sd(transformed_df$X1)/sd(transformed_df$Y) * b[1]; b1_star
# b2 = (sY/s2)b2*
# b2* = (s2/sY)b2
b2_star <- sd(transformed_df$X2)/sd(transformed_df$Y) * b[2]; b2_star
```